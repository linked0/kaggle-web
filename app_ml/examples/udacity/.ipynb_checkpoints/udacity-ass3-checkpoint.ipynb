{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cleansed training data: ', (200000, 28, 28), (200000,))\n",
      "('cleansed validation data: ', (8933, 28, 28), (8933,))\n",
      "('cleansed test data: ', (8639, 28, 28), (8639,))\n",
      "Training set: (200000, 784), (200000, 10)\n",
      "Validation set: (8933, 784), (8933, 10)\n",
      "Test set: (8639, 784), (8639, 10)\n"
     ]
    }
   ],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "pickle_file = '../large_data/udacity/notMNIST2.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'rb')\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('unable to load file ', pickle_file, ':', e)\n",
    "\n",
    "train_dataset = data['train_dataset']\n",
    "train_labels = data['train_labels']\n",
    "valid_dataset = data['valid_dataset']\n",
    "valid_labels = data['valid_labels']\n",
    "test_dataset = data['test_dataset']\n",
    "test_labels = data['test_labels']\n",
    "\n",
    "print('cleansed training data: ', train_dataset.shape, train_labels.shape)\n",
    "print('cleansed validation data: ', valid_dataset.shape, valid_labels.shape)\n",
    "print('cleansed test data: ', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "batch_size = 128\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set: %s, %s' % (train_dataset.shape, train_labels.shape))\n",
    "print('Validation set: %s, %s' % (valid_dataset.shape, valid_labels.shape))\n",
    "print('Test set: %s, %s' %(test_dataset.shape, test_labels.shape))\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) /\n",
    "            predictions.shape[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step: 0: 1949.999268\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 31.1%\n",
      "Minibatch loss at step: 200: 869.679810\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step: 400: 473.261353\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step: 600: 257.418182\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step: 800: 140.108185\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step: 1000: 77.046410\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step: 1200: 42.250282\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step: 1400: 23.448168\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step: 1600: 13.321760\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step: 1800: 7.615784\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step: 2000: 4.337368\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step: 2200: 2.789174\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step: 2400: 1.777931\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step: 2600: 1.304821\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step: 2800: 1.004867\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step: 3000: 0.861021\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.3%\n",
      "Test accuracy: 91.2%\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 1024\n",
    "num_steps = 3001\n",
    "beta = 0.01\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights_l1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    biases_l1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "    logits_l1 = tf.matmul(tf_train_dataset, weights_l1) + biases_l1\n",
    "    logits_l1 = tf.nn.relu(logits_l1)\n",
    "#     logits_l1 = tf.nn.dropout(logits_l1, keep_prob)\n",
    "    \n",
    "    weights_l2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    biases_l2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    logits_l2 = tf.matmul(logits_l1, weights_l2) + biases_l2\n",
    "\n",
    "    regularizers = tf.nn.l2_loss(weights_l1) + tf.nn.l2_loss(weights_l2)\n",
    "    regularizers = 0.5 * regularizers * beta\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits_l2, tf_train_labels)) + regularizers\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.5\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           1000, 0.98, staircase=True)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits_l2)\n",
    "    valid_layer1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_l1) + biases_l1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_layer1, weights_l2) + biases_l2)\n",
    "    test_layer1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_l1) + biases_l1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_layer1, weights_l2) + biases_l2)\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 200 == 0):\n",
    "            print('Minibatch loss at step: %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
